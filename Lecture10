Linearity of Expectation 

Let T=X+Y . show E(T) = E(X)+ E(Y). 

proof. E(T) = \sum t*P(T=t) =? \sum xP(X=x)+ \sum yP(Y=y) 
P(T=t)=\sum_x P(T=t|X=x)P(X=x) (?x not available)

E(X)= \sum_x xP(X=x)(grouped)  = \sum_x X(s)P(s)(ungrouped)

Proof of linearity( discrete case) 

E(T) = \sum_s (X+Y)(s)P(s) = \sum_s {X(s)+Y(s)} *P(s)
     = \sum_s X(s)P(s)+ Y(s)P(s) =E(X)+E(Y)
     
Similarly, E(cX)=cE(X) if c is a constant. 

Here is a extreme case of denpendence: X=Y ,Then E(X+Y)=E(X)+E(Y) = 2E(X) 

Negative Binomial: parameters {r,p} 
Story: indenpendent Ber(p) trials, #of failures befor the r'rh success.

PMF:  P(X=n)= (n+r-1,r-1)p^(r-1)q^(n)*p =(n+r-1,r-1)p^r*q^n n=0,1,2,...
E(X)= E(X_1+X_2+...+X_r) X_j is the number of failures between (j-1) and j's success. 
X_j ~ Geom(p) though X_j and X_i are not independent, but we only want to get the expecation,
so, this is enough to caculate the expectation of X. 

So, E(X) =\sum_j E(X_j)= r*q/p. 





X~FS(p) time until 1st success, counting the success. 
Let Y=X-1, then Y~ Geom(p) E(X) = E(Y)+1= q/p+1 = 1/p. 

Examples:

Putram: Random permutation of 1,2,3....n. when n>=2; 
Question: Find expected number of local maxima? 

Let I_j be the indicator r.v. of position j having a local max, 1<=j<=n; 
Question E(I_1+I_2+...+I_n) = \sum_j E(I_j) =(n-2)/3 + 2/2 = (n+1)/3 
     
St. Peterburg Paradox: Get 2^x dollors where x is the number of flips of fair coin 
until first H, including the success. 

Y=2^x, find E(Y) 
E(Y) = \sum_{k=1}^{\inf} 2^k/2^k = \sum_{k=1}^{\inf} =\inf 

bound at 2^40. Then \sum_{k=1}^{40} 2^k /2^k =40. 

Caution! 
E(2^X) != 2^{E(X)) . 
     
     
